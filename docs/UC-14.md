# Use Case UC-14: View Forecast Accuracy and Compare Predictions to Actuals

**Goal in Context**: Allow city planners and performance analysts to review recent forecast accuracy and compare past predictions to actual outcomes so they can evaluate performance and build trust in the system.  
**Scope**: Operations Analytics System  
**Level**: User Goal  
**Primary Actor**: City Planner or Performance Analyst  
**Secondary Actors**: Forecasting Engine, Evaluation Module, Data Storage System, Visualization Module  
**Trigger**: User opens the forecast performance analysis interface.

---

## Success End Condition
* Forecast accuracy metrics and comparisons between past predictions and actual outcomes are displayed.  
* User can interpret system performance over time.

## Failed End Condition
* Accuracy data cannot be retrieved or displayed; system logs the failure and shows an error state.

---

## Preconditions
* Historical forecasts and actual demand data are stored.  
* Evaluation metrics have been computed.  
* Visualization services are operational.

---

## Main Success Scenario
1. User accesses the forecast performance analysis interface.  
2. System retrieves stored historical forecasts.  
3. System retrieves corresponding actual demand data.  
4. System retrieves or computes forecast accuracy metrics.  
5. System aligns forecasts and actuals for comparison.  
6. System prepares data for visualization.  
7. System renders charts or tables showing prediction vs. actual comparisons and accuracy metrics.  
8. User reviews the displayed performance information.  
9. System logs successful retrieval and visualization.

---

## Extensions
* **2a**: Historical forecast data unavailable  
  * 2a1: System logs missing forecast data.  
  * 2a2: System displays error state.  

* **3a**: Actual demand data unavailable  
  * 3a1: System logs missing actual data.  
  * 3a2: System displays error state.  

* **4a**: Evaluation metrics missing  
  * 4a1: System logs missing metrics.  
  * 4a2: System displays comparison without metrics if possible.  

* **7a**: Visualization rendering error  
  * 7a1: System logs rendering failure.  
  * 7a2: System displays error state.

---

## Related Information
* **Priority**: Medium  
* **Frequency**: Periodically or as needed for evaluation  
* **Open Issues**: Standard reporting periods and metric definitions not yet finalized.

# UC-14 Fully Dressed Scenario Narratives  
**Use Case:** View Forecast Accuracy and Compare Predictions to Actuals  

---

## Main Success Scenario Narrative

A city planner or performance analyst wants to understand how well the forecasting system has performed recently. The user opens the forecast performance analysis interface.

The system retrieves stored historical forecast outputs for previous periods. It also retrieves the corresponding actual demand data for those same time windows.

The system accesses precomputed evaluation metrics such as MAE, RMSE, or MAPE, or calculates them if necessary.

The system aligns past predictions with actual outcomes along common time intervals and service categories.

The visualization module prepares charts or tables that display forecasted values versus actual demand, along with accuracy metrics.

The user reviews the information to assess how accurate the forecasts have been and whether performance is improving or declining over time.

The system logs successful data retrieval and visualization.

---

## Alternative Scenario 2a — Historical Forecast Data Unavailable

The user opens the performance interface, but the system cannot retrieve stored historical forecasts due to missing data or retrieval failure.

The system logs the missing forecast data condition.

An error state is displayed instead of comparison charts.

---

## Alternative Scenario 3a — Actual Demand Data Unavailable

Historical forecasts are retrieved successfully, but actual demand data for the same periods is missing.

The system logs the missing actual data condition.

The system cannot produce valid comparisons and displays an error state.

---

## Alternative Scenario 4a — Evaluation Metrics Missing

The system retrieves forecasts and actual data, but evaluation metrics have not been computed or stored.

The system logs the missing metrics condition.

If possible, the system displays prediction-versus-actual charts without summary metrics; otherwise, it displays an error state.

---

## Alternative Scenario 7a — Visualization Rendering Error

All required data is retrieved successfully. However, the visualization module fails during chart or table rendering.

The system logs the rendering error and displays an error state.

No performance comparison visuals are shown.

---

## Key Behavioral Theme Across All Alternatives

Across all scenarios:

- The system avoids presenting misleading or incomplete performance information  
- Missing data or processing issues are clearly logged  
- Users either see valid performance insights or a clear error/limitation message  
- Trust and transparency are prioritized in performance evaluation
