# Use Case UC-02: Validate and Deduplicate Ingested Data

**Goal in Context**: Ensure that newly ingested data passes schema validation and deduplication rules so that only reliable, consistent data is made available for forecasting and dashboards.  
**Scope**: Operations Analytics System  
**Level**: User Goal  
**Primary Actor**: City Planner  
**Secondary Actors**: Data Ingestion Service, Validation Engine, Data Storage System  
**Trigger**: New dataset is ingested into the system.

---

## Success End Condition
* The dataset passes schema validation and deduplication checks.  
* Cleaned and verified data is stored and marked as approved for downstream use.

## Failed End Condition
* The dataset is rejected due to schema or duplication issues.  
* No invalid data is made available for forecasting or dashboards.

---

## Preconditions
* A dataset has been ingested into the system.  
* Validation rules and schema definitions are configured.  
* Deduplication rules are defined and active.  
* Storage and processing services are operational.

---

## Main Success Scenario
1. Data Ingestion Service submits a newly ingested dataset for processing.  
2. System applies schema validation rules to the dataset.  
3. System confirms all required fields, formats, and data types are correct.  
4. System applies deduplication rules to detect duplicate records.  
5. System removes or consolidates duplicates according to system policy.  
6. System marks the dataset as validated and clean.  
7. System stores the validated dataset in the Data Storage System.  
8. System marks the dataset as available for forecasting models and dashboards.  
9. System logs successful validation and deduplication.

---

## Extensions
* **2a**: Schema validation fails  
  * 2a1: System identifies schema errors.  
  * 2a2: System logs validation failure details.  
  * 2a3: System rejects the dataset.  

* **4a**: Deduplication process fails  
  * 4a1: System logs processing error.  
  * 4a2: System marks the run as failed.  

* **5a**: Excessive duplicate rate detected  
  * 5a1: System flags dataset for review.  
  * 5a2: System prevents dataset from being marked as clean.  

* **7a**: Storage failure  
  * 7a1: System logs storage error.  
  * 7a2: System marks the run as failed.

---

## Related Information
* **Priority**: High  
* **Frequency**: Occurs whenever new data is ingested  
* **Open Issues**: Threshold for acceptable duplicate rates not yet defined.

# UC-02 Fully Dressed Scenario Narratives  
**Use Case:** Validate and Deduplicate Ingested Data  

---

## Main Success Scenario Narrative

A new dataset is ingested into the system through the data ingestion pipeline. Once ingestion completes, the system automatically routes the dataset to the validation engine.

The system begins schema validation by comparing the dataset structure against predefined schema rules. It checks that required columns are present, data types match expectations, formats (such as dates and IDs) are correct, and no structural corruption exists.

All schema checks pass successfully. The system proceeds to the deduplication phase. It scans the dataset for duplicate records based on defined keys such as service request ID, timestamp, and location.

Where duplicates are found, the system resolves them according to policy — for example, keeping the most recent record or merging non-conflicting fields. The resulting dataset contains only unique, clean records.

The system marks the dataset as validated and clean. It stores the processed dataset in the operational data repository and designates it as approved for forecasting and dashboard use.

Finally, the system logs successful completion, including the number of records processed, duplicates removed, and validation status. Downstream analytics processes now use this reliable dataset.

---

## Alternative Scenario 2a — Schema Validation Failure

After ingestion, the system begins schema validation. During this process, the system detects issues such as missing required fields, incorrect data types, malformed dates, or structural inconsistencies.

Because the dataset does not conform to the required schema, validation fails. The system does not proceed to deduplication.

The system logs detailed validation errors, including which fields failed and why. The dataset is rejected and not stored as a validated dataset.

Previously validated datasets remain active for dashboards and forecasting to ensure continuity.

---

## Alternative Scenario 4a — Deduplication Process Failure

The dataset passes schema validation successfully and moves to deduplication. During this step, a processing error occurs — for example, a rule conflict, system exception, or resource failure.

The system stops the deduplication process and logs the error details. Because deduplication did not complete, the dataset cannot be considered clean.

The system marks the run as failed and does not store the dataset as validated. The prior dataset remains active for downstream use.

---

## Alternative Scenario 5a — Excessive Duplicate Rate Detected

The dataset passes schema validation and enters deduplication. While processing, the system detects that the proportion of duplicate records exceeds an acceptable threshold.

Although duplicates could technically be removed, the unusually high rate suggests a potential upstream data quality issue.

The system flags the dataset for review, logs the condition, and prevents the dataset from being marked as clean or available for analytics.

The previous validated dataset remains active.

---

## Alternative Scenario 7a — Storage Failure

The dataset successfully passes validation and deduplication. When the system attempts to store the cleaned dataset, the storage system fails — due to outage, capacity limits, or permission errors.

The system logs the storage failure and marks the run as failed. Because the dataset was not safely stored, it is not marked as current.

The previously validated dataset continues to serve dashboards and forecasting processes.

---

## Key Behavioral Theme Across All Alternatives

In every failure scenario:

- No unvalidated or partially processed data becomes active  
- Previously validated data remains in use  
- Failures are logged for monitoring and investigation  
- System reliability and data integrity are prioritized over freshness
