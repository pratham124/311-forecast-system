# Use Case UC-06: Evaluate Forecasting Engine Against Baselines

**Goal in Context**: Evaluate the forecasting engine against baseline methods (e.g., seasonal naïve, moving average) to determine whether the system provides improved predictive performance.  
**Scope**: Operations Analytics System  
**Level**: User Goal  
**Primary Actor**: City Planner 
**Secondary Actors**: Forecasting Engine, Baseline Model Module, Evaluation Module, Data Storage System  
**Trigger**: City Planner initiates a model evaluation or a scheduled evaluation cycle occurs.

---

## Success End Condition
* Forecasting engine performance metrics are calculated and compared with baseline methods.  
* Evaluation results are stored and available for review.

## Failed End Condition
* Evaluation cannot be completed; system logs the failure and retains previous evaluation results.

---

## Preconditions
* Historical demand data is available.  
* Baseline models are configured and operational.  
* Forecasting engine is operational.  
* Evaluation metrics and criteria are defined.

---

## Main Success Scenario
1. City Planner initiates evaluation or a scheduled evaluation process begins.  
2. System retrieves historical data and recent forecast outputs.  
3. System generates baseline forecasts using predefined baseline methods.  
4. System compares forecasting engine predictions with baseline predictions against actual outcomes.  
5. System computes evaluation metrics (e.g., MAE, RMSE, MAPE).  
6. System aggregates results across service categories and time periods.  
7. System stores evaluation results in the Data Storage System.  
8. System makes results available for analyst review.  
9. System logs successful evaluation completion.

---

## Extensions

* **2a**: Required data unavailable  
  * 2a1: System logs missing data issue.  
  * 2a2: System marks evaluation as failed.  

* **3a**: Baseline model failure  
  * 3a1: System logs baseline generation error.  
  * 3a2: System marks evaluation as failed.  

* **4a**: Forecast output missing  
  * 4a1: System logs missing forecast data.  
  * 4a2: System marks evaluation as failed.  

* **5a**: Evaluation metric computation failure  
  * 5a1: System logs metric calculation issue (e.g., invalid values, divide-by-zero, insufficient data).  
  * 5a2: System excludes affected metrics or categories from results.  
  * 5a3: System continues evaluation for remaining valid metrics.  

* **7a**: Storage failure  
  * 7a1: System logs storage error.  
  * 7a2: System marks evaluation as failed.

---

## Related Information
* **Priority**: Medium  
* **Frequency**: Periodic (e.g., weekly or monthly)  
* **Open Issues**: Definition of performance thresholds and reporting format not yet finalized.

---

# UC-06 Fully Dressed Scenario Narratives  
**Use Case:** Evaluate Forecasting Engine Against Baselines  

---

## Main Success Scenario Narrative

A city planner wants to confirm that the forecasting engine delivers better predictions than simple baseline methods. At the scheduled evaluation time, or after manually initiating an evaluation, the system begins the model assessment process.

The system retrieves historical demand data along with recent forecast outputs generated by the forecasting engine. These datasets include actual outcomes and predicted values over the same time periods.

Next, the system runs baseline forecasting methods such as seasonal naïve and moving average models using the same historical data. These baseline predictions provide reference performance levels.

The evaluation module compares the forecasting engine’s predictions and the baseline predictions against actual demand values. It calculates evaluation metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE).

The system aggregates results across service categories and relevant time windows to produce a comprehensive performance summary.

The evaluation results are stored in the operational data repository and made available for review through reporting or analytics interfaces.

The system logs successful completion of the evaluation, including metrics, timestamps, and model identifiers. The city planner can now determine whether the forecasting engine provides added value.

---

## Alternative Scenario 2a — Required Data Unavailable

The evaluation process begins, but the system cannot retrieve required historical data or recent forecast outputs.

Because necessary data is missing, the system cannot run baseline models or calculate evaluation metrics.

The system logs the missing data issue and marks the evaluation attempt as failed. The previous evaluation results remain available for reference.

---

## Alternative Scenario 3a — Baseline Model Failure

The system successfully retrieves historical data and forecast outputs. However, during execution of a baseline model (e.g., moving average), an error occurs due to configuration issues, processing errors, or resource limitations.

The system logs the baseline generation failure and stops the evaluation process.

No new evaluation results are produced. Previous evaluation results remain available.

---

## Alternative Scenario 4a — Forecast Output Missing

The system retrieves historical data, but corresponding forecast outputs from the forecasting engine are unavailable or incomplete.

Without forecast predictions, comparison against baselines cannot proceed.

The system logs the missing forecast data and marks the evaluation as failed.

---

## Alternative Scenario 5a — Metric Computation Failure

Data and model outputs are retrieved successfully, but one or more evaluation metrics cannot be computed due to invalid values, divide-by-zero, or insufficient data.

The system logs the issue and excludes affected metrics or categories. Remaining valid metrics are computed and aggregated.

Results are stored with notes about excluded metrics.

---

## Alternative Scenario 7a — Storage Failure

The evaluation completes successfully and performance metrics are calculated. However, when the system attempts to store the evaluation results, a storage failure occurs.

The system logs the storage error and marks the evaluation run as failed.

Although the evaluation was computed, it is not saved as the current official evaluation. Previous stored results remain available.

---

## Key Behavioral Theme Across All Alternatives

Across all failure scenarios:

- No partial or unreliable evaluation results replace valid prior results  
- Previous evaluations remain available for reference  
- Failures are logged for monitoring and investigation  
- Evaluation integrity and comparability are prioritized over incomplete reporting
